{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1JvTVjrdXI6"
      },
      "source": [
        "# Download Dataset from GitHub to Google Colaboratory\n",
        "\n",
        "The custom dataset has been uploaded to the GitHub repository to clone the dataset and use it in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJQr1VMhPSaZ"
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/Intelligent-Systems-TCZR/HOG.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOBqLpTL4TSt"
      },
      "source": [
        "# Show how HOG works\n",
        "Input an image, and it will turn the image to grey and start to calculate the gradients.\n",
        "\n",
        "In the end, it will show the image after implement the HOG feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPGNg3Xv_xtF"
      },
      "source": [
        "# Visualise HOG\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io\n",
        "from skimage.feature import hog\n",
        "from skimage import data, color, exposure\n",
        "from PIL import Image\n",
        "\n",
        "# Input the image\n",
        "img = io.imread(r\"/content/HOG/sample/people.jpg\")\n",
        "\n",
        "# Turn the image to grey\n",
        "image = color.rgb2gray(img)\n",
        "\n",
        "# creating hog features\n",
        "fd, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),\n",
        "                    cells_per_block=(1, 1), visualize=True)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
        "\n",
        "ax1.axis('off')\n",
        "ax1.imshow(image, cmap=plt.cm.gray)\n",
        "ax1.set_title('Input image')\n",
        "ax1.set_adjustable('box')\n",
        "\n",
        "# Rescale histogram for better display\n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 0.02))\n",
        "\n",
        "ax2.axis('off')\n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
        "ax2.set_title('Histogram of Oriented Gradients')\n",
        "ax1.set_adjustable('box')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-wUin4q8918"
      },
      "source": [
        "#Train the SVM model\n",
        "\n",
        "Here use two types of datasets to train the model, one is the positive samples (Human images), and the other is the negative samples (Non-Human images)\n",
        "\n",
        "The dataset will split into 80% training dataset and 20% test dataset to see how well the model performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQCQn0e6G34d"
      },
      "source": [
        "# Training\n",
        "# Importing the necessary modules:\n",
        "\n",
        "from skimage.feature import hog\n",
        "from skimage.transform import pyramid_gaussian\n",
        "from skimage.io import imread\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage import color\n",
        "from imutils.object_detection import non_max_suppression\n",
        "import imutils\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image # This will be used to read/modify images (can be done via OpenCV too)\n",
        "from numpy import *\n",
        "\n",
        "# define parameters of HOG feature extraction\n",
        "orientations = 9\n",
        "pixels_per_cell = (8, 8)\n",
        "cells_per_block = (2, 2)\n",
        "threshold = .3\n",
        "\n",
        "\n",
        "# define path to images:\n",
        "\n",
        "# This is the path of our positive input dataset\n",
        "pos_im_path = r\"/content/HOG/train/positive\" \n",
        "# define the same for negatives\n",
        "neg_im_path= r\"/content/HOG/train/negative\"\n",
        "\n",
        "# read the image files:\n",
        "pos_im_listing = os.listdir(pos_im_path) # it will read all the files in the positive image path (so all the required images)\n",
        "neg_im_listing = os.listdir(neg_im_path)\n",
        "num_pos_samples = size(pos_im_listing) # simply states the total no. of images\n",
        "num_neg_samples = size(neg_im_listing)\n",
        "print(num_pos_samples) # prints the number value of the no.of samples in positive dataset\n",
        "print(num_neg_samples)\n",
        "data= []\n",
        "labels = []\n",
        "\n",
        "# compute HOG features and label them:\n",
        "\n",
        "for file in pos_im_listing: #this loop enables reading the files in the pos_im_listing variable one by one\n",
        "    img = Image.open(pos_im_path + '/' + file) # open the file\n",
        "    gray = img.convert('L') # convert the image into single channel i.e. RGB to grayscale\n",
        "    # calculate HOG for positive features\n",
        "    fd = hog(gray, orientations, pixels_per_cell, cells_per_block, block_norm='L2', feature_vector=True)# fd= feature descriptor\n",
        "    data.append(fd)\n",
        "    labels.append(1)\n",
        "    \n",
        "# Same for the negative images\n",
        "for file in neg_im_listing: #this loop enables reading the files in the neg_im_listing variable one by one\n",
        "    img= Image.open(neg_im_path + '/' + file)\n",
        "    gray= img.convert('L')\n",
        "    # Now we calculate the HOG for negative features\n",
        "    fd = hog(gray, orientations, pixels_per_cell, cells_per_block, block_norm='L2', feature_vector=True) \n",
        "    data.append(fd)\n",
        "    labels.append(0)\n",
        "# encode the labels, converting them from strings to integers\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "\n",
        "# Partitioning the data into training and testing splits, using 80%\n",
        "# of the data for training and the remaining 20% for testing\n",
        "print(\" Constructing training/testing split...\")\n",
        "(trainData, testData, trainLabels, testLabels) = train_test_split(\n",
        "\tnp.array(data), labels, test_size=0.20, random_state=42)\n",
        "#%% Train the linear SVM\n",
        "print(\" Training Linear SVM classifier...\")\n",
        "model = LinearSVC()\n",
        "model.fit(trainData, trainLabels)\n",
        "#%% Evaluate the classifier\n",
        "print(\" Evaluating classifier on test data ...\")\n",
        "predictions = model.predict(testData)\n",
        "print(classification_report(testLabels, predictions))\n",
        "\n",
        "# Save the model:\n",
        "joblib.dump(model, 'model_name.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HACbA9wnB-qA"
      },
      "source": [
        "#Run the HOG feature on Test Video and Image\n",
        "\n",
        "In this section, we will input a video for it to detect the pedestrian on the road.\n",
        " \n",
        "It will capture every frame of the video and display it whenever a human is detected. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc8tkwp_FP_k"
      },
      "source": [
        "# Testing\n",
        "# Modified from OpenCV HOG person detector\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import sys\n",
        "from glob import glob\n",
        "import itertools as it\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def inside(r, q):\n",
        "    rx, ry, rw, rh = r\n",
        "    qx, qy, qw, qh = q\n",
        "    return rx > qx and ry > qy and rx + rw < qx + qw and ry + rh < qy + qh\n",
        "\n",
        "def draw_detections(img, rects, thickness = 1):\n",
        "    for x, y, w, h in rects:\n",
        "        # the HOG detector returns slightly larger rectangles than the real objects.\n",
        "        # so we slightly shrink the rectangles to get a nicer output.\n",
        "        pad_w, pad_h = int(0.15*w), int(0.05*h)\n",
        "        cv2.rectangle(img, (x+pad_w, y+pad_h), (x+w-pad_w, y+h-pad_h), (0, 0, 255), thickness)\n",
        "\n",
        "hog = cv2.HOGDescriptor()\n",
        "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
        "''' the above code uses a pretrained SVM via HOG descriptors provided by the open cv database.\n",
        "This database is limited to the training it has performed hence cannot be used in any other angle other than perp. to the centroid\n",
        "Thus if you want to implement the HOG + SVM method, you'll have to train your own SVM with your own data'''\n",
        "cap= cv2.VideoCapture(\"/content/HOG/test/video/video1.mp4\")\n",
        "# the above code uses the OpenCV library to capture video frames from the video (you can also input an image)\n",
        "\n",
        "while True:\n",
        "    #running an infinite loop so that the process is run real time.\n",
        "    ret, img = cap.read() # reading the frames produced from the video in 'img' an then returning them using the 'ret' function.\n",
        "    found, w = hog.detectMultiScale(img, winStride=(8,8), padding=(32,32), scale=1.05) # describing the parameters of HOG and returning them as a Human found function in 'found'\n",
        "    found_filtered = [] #filtering the found human... to further improve visualisation (uses Gaussian filter for eradication of errors produced by luminescence.\n",
        "    for ri, r in enumerate(found):\n",
        "        for qi, q in enumerate(found):\n",
        "            if ri != qi and inside(r, q):\n",
        "                break\n",
        "            else:\n",
        "                found_filtered.append(r)\n",
        "        draw_detections(img, found) # using the predefined bounding box to encapsulate the human detected within the bounding box.\n",
        "        draw_detections(img, found_filtered, 3) # further filtering the box to improve visualisation.\n",
        "    cv2_imshow(img) # finally showing the resulting image captured from the video.\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
